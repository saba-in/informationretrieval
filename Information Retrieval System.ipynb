{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System - (Four Stories)\n",
    "\n",
    "#### 1.  AIR CONDITIONING\n",
    "#### 2. THE FABLE OF THE ANT AND THE CRICKET\n",
    "#### 3. THE SEVEN OLD SAMURAI\n",
    "#### 4. THE THREE WISHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Story 1 \"AIR CONDITIONING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"aircon.txt\",\"r\") \n",
    "\n",
    "# read all lines at once\n",
    "text1 = file1.read()\n",
    "\n",
    "# close the file\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing \"Air Conditioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text1)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'conditioning', 'my', 'telephone', 'receiver', 'slams', 'down', 'on', 'its', 'cradle', 'im', 'upset', 'i', 'am', 'soaked', 'to', 'the', 'skin', 'sweat', 'runs', 'from', 'my', 'brow', 'the', 'air', 'conditioner', 'that', 'i', 'so', 'naively', 'entrusted', 'to', 'the', 'yellow', 'pages', 'repair', 'shop', 'is', 'delayed', 'another', 'two', 'weeks', 'i', 'could', 'have', 'it', 'back', 'tomorrow', 'i', 'was', 'told', 'if', 'i', 'happen', 'to', 'have', 'a', 'compressor', 'relief', 'control', 'valve', 'sensor', 'assembly', 'part', 'number', 'in', 'my', 'pocket', 'the', 'repairman', 'is', 'a', 'funny', 'fellow', 'very', 'funny', 'its', 'a', 'bit', 'stuffy', 'in', 'here', 'my', 'secretary', 'says', 'in', 'an', 'attempt', 'to', 'explain', 'her', 'entering', 'my', 'office', 'this', 'is', 'obvious', 'of', 'course', 'as']\n"
     ]
    }
   ],
   "source": [
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data1 = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Story 2 \"The Fable of the Ant and the Cricket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open(\"antcrick.txt\",\"r\") \n",
    "\n",
    "# read all lines at once\n",
    "text2 = file2.read()\n",
    "\n",
    "# close the file\n",
    "file2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing \"The Fable of the Ant and the Cricket\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fable', 'of', 'the', 'ant', 'and', 'the', 'cricket', 'once', 'upon', 'a', 'time', 'one', 'hot', 'summer', 'a', 'cricket', 'sang', 'cheerfully', 'on', 'the', 'branch', 'of', 'a', 'tree', 'while', 'down', 'below', 'a', 'long', 'line', 'of', 'ants', 'struggled', 'damely', 'under', 'the', 'weight', 'of', 'their', 'load', 'of', 'grains', 'and', 'between', 'one', 'song', 'and', 'the', 'next', 'the', 'cricket', 'spoke', 'to', 'the', 'ants', 'why', 'are', 'you', 'working', 'so', 'hard', 'come', 'into', 'the', 'shade', 'away', 'from', 'the', 'sun', 'and', 'sing', 'a', 'song', 'with', 'me', 'but', 'the', 'tireless', 'ants', 'went', 'on', 'with', 'the', 'work', 'we', 'ca', 'nt', 'do', 'that', 'they', 'said', 'we', 'must', 'store', 'away', 'food', 'for', 'the', 'winter']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text2)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data2 = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Story 3 \"THE SEVEN OLD SAMURAI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = open(\"7oldsamr.txt\",\"r\") \n",
    "\n",
    "# read all lines at once\n",
    "text3 = file3.read()\n",
    "\n",
    "# close the file\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing \"THE SEVEN OLD SAMURAI\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'seven', 'old', 'samurai', 'once', 'upon', 'a', 'time', 'in', 'far', 'off', 'japan', 'a', 'band', 'of', 'fierce', 'robbers', 'had', 'their', 'hiding', 'place', 'on', 'top', 'of', 'a', 'mountain', 'almost', 'always', 'covered', 'with', 'grey', 'clouds', 'windswept', 'and', 'battered', 'by', 'storms', 'the', 'robbers', 'lived', 'in', 'a', 'large', 'cave', 'where', 'they', 'had', 'piled', 'their', 'spoils', 'now', 'and', 'again', 'they', 'went', 'down', 'the', 'mountain', 'attacked', 'a', 'village', 'murdered', 'the', 'poor', 'folk', 'they', 'chanced', 'upon', 'stole', 'whatever', 'they', 'could', 'lay', 'hands', 'on', 'and', 'burned', 'it', 'to', 'the', 'ground', 'wherever', 'the', 'robbers', 'passed', 'there', 'was', 'nothing', 'but', 'smoking', 'ruins', 'weeping', 'men', 'and', 'women', 'misery', 'mournlng', 'and', 'desolation', 'the']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text3)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data3 = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Story 4 \"THE THREE WISHES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file4 = open(\"3wishes.txt\",\"r\") \n",
    "\n",
    "# read all lines at once\n",
    "text4 = file4.read()\n",
    "\n",
    "# close the file\n",
    "file4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing \"THE THREE WISHES\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'three', 'wishes', 'once', 'upon', 'a', 'time', 'a', 'woodcutter', 'lived', 'happily', 'with', 'his', 'wife', 'in', 'a', 'pretty', 'little', 'log', 'cabin', 'in', 'the', 'middle', 'of', 'a', 'thick', 'forest', 'each', 'morning', 'he', 'set', 'off', 'singing', 'to', 'work', 'and', 'when', 'he', 'came', 'home', 'in', 'the', 'evening', 'a', 'plate', 'of', 'hot', 'steaming', 'soup', 'was', 'always', 'waiting', 'for', 'him', 'one', 'day', 'however', 'he', 'had', 'a', 'strange', 'surprise', 'he', 'came', 'upon', 'a', 'big', 'fir', 'tree', 'with', 'strange', 'open', 'holes', 'on', 'the', 'trunk', 'it', 'looked', 'somehow', 'different', 'from', 'the', 'other', 'trees', 'and', 'just', 'as', 'he', 'was', 'about', 'to', 'chop', 'it', 'down', 'the', 'alarmed', 'face', 'of', 'an', 'elf']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text4)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data4 = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [data1, data2, data3, data4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inverted Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inv_indx = defaultdict(list)\n",
    "for idx, text in enumerate(data):\n",
    "    for word in text:\n",
    "        inv_indx[word].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'air': [0, 0, 0, 3],\n",
       "             'conditioning': [0],\n",
       "             'telephone': [0],\n",
       "             'receiver': [0],\n",
       "             'slams': [0],\n",
       "             'cradle': [0],\n",
       "             'im': [0],\n",
       "             'upset': [0, 3],\n",
       "             'soaked': [0],\n",
       "             'skin': [0],\n",
       "             'sweat': [0, 0],\n",
       "             'runs': [0],\n",
       "             'brow': [0],\n",
       "             'conditioner': [0, 0],\n",
       "             'naively': [0],\n",
       "             'entrusted': [0],\n",
       "             'yellow': [0, 1],\n",
       "             'pages': [0],\n",
       "             'repair': [0],\n",
       "             'shop': [0, 0],\n",
       "             'delayed': [0],\n",
       "             'another': [0, 3],\n",
       "             'two': [0],\n",
       "             'weeks': [0, 1, 1],\n",
       "             'could': [0, 1, 2, 2, 3, 3, 3],\n",
       "             'back': [0],\n",
       "             'tomorrow': [0],\n",
       "             'told': [0, 3],\n",
       "             'happen': [0],\n",
       "             'compressor': [0],\n",
       "             'relief': [0],\n",
       "             'control': [0],\n",
       "             'valve': [0],\n",
       "             'sensor': [0],\n",
       "             'assembly': [0],\n",
       "             'part': [0],\n",
       "             'number': [0],\n",
       "             'pocket': [0],\n",
       "             'repairman': [0],\n",
       "             'funny': [0, 0, 3],\n",
       "             'fellow': [0],\n",
       "             'bit': [0, 3],\n",
       "             'stuffy': [0],\n",
       "             'secretary': [0, 0],\n",
       "             'says': [0],\n",
       "             'attempt': [0],\n",
       "             'explain': [0],\n",
       "             'entering': [0],\n",
       "             'office': [0, 0, 0],\n",
       "             'obvious': [0],\n",
       "             'course': [0],\n",
       "             'nary': [0],\n",
       "             'breeze': [0],\n",
       "             'wafts': [0],\n",
       "             'threefoot': [0],\n",
       "             'square': [0],\n",
       "             'hole': [0, 0, 3],\n",
       "             'wall': [0, 0],\n",
       "             'appeared': [0, 3],\n",
       "             'synchronization': [0],\n",
       "             'disappearance': [0],\n",
       "             'goes': [0],\n",
       "             'thermostat': [0],\n",
       "             'checks': [0],\n",
       "             'temperature': [0, 0],\n",
       "             'adjusts': [0],\n",
       "             'setting': [0],\n",
       "             'fourth': [0],\n",
       "             'time': [0, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "             'morning': [0, 1, 3],\n",
       "             'shaking': [0],\n",
       "             'head': [0, 1, 2, 3],\n",
       "             'frustration': [0],\n",
       "             'try': [0, 1, 3],\n",
       "             'decipher': [0],\n",
       "             'overdue': [0],\n",
       "             'report': [0],\n",
       "             'blurred': [0],\n",
       "             'illegibility': [0],\n",
       "             'excellent': [0],\n",
       "             'typist': [0],\n",
       "             'best': [0],\n",
       "             'ive': [0],\n",
       "             'ever': [0, 3],\n",
       "             'completely': [0],\n",
       "             'fulfilling': [0],\n",
       "             'secretarial': [0],\n",
       "             'duties': [0],\n",
       "             'otherwise': [0],\n",
       "             'keeps': [0],\n",
       "             'although': [0],\n",
       "             'nature': [0],\n",
       "             'curious': [0],\n",
       "             'man': [0, 3],\n",
       "             'personal': [0, 0, 0],\n",
       "             'matters': [0],\n",
       "             'us': [0, 2, 2],\n",
       "             'never': [0],\n",
       "             'discussed': [0],\n",
       "             'however': [0, 3, 3],\n",
       "             'increase': [0],\n",
       "             'attire': [0, 0],\n",
       "             'late': [0, 3],\n",
       "             'become': [0],\n",
       "             'remarkable': [0],\n",
       "             'increasing': [0],\n",
       "             'skimpiness': [0],\n",
       "             'attempted': [0],\n",
       "             'fill': [0, 1],\n",
       "             'wadded': [0],\n",
       "             'papers': [0],\n",
       "             'rags': [0],\n",
       "             'proven': [0],\n",
       "             'ineffective': [0],\n",
       "             'thanks': [0, 2],\n",
       "             'active': [0],\n",
       "             'flocks': [0],\n",
       "             'nesting': [0],\n",
       "             'pigeons': [0],\n",
       "             'neighborhood': [0],\n",
       "             'last': [0, 1, 1, 2, 2, 2, 3],\n",
       "             'spring': [0],\n",
       "             'reeceived': [0],\n",
       "             'bill': [0, 0],\n",
       "             'local': [0, 0],\n",
       "             'supply': [0, 0],\n",
       "             'rather': [0, 1],\n",
       "             'badly': [0],\n",
       "             'smeared': [0],\n",
       "             'notice': [0],\n",
       "             'something': [0],\n",
       "             'furniture': [0],\n",
       "             'recently': [0],\n",
       "             'gave': [0],\n",
       "             'clue': [0],\n",
       "             'secretarys': [0],\n",
       "             'life': [0, 3],\n",
       "             'recent': [0],\n",
       "             'change': [0],\n",
       "             'quite': [0, 3],\n",
       "             'revealing': [0],\n",
       "             'confirms': [0],\n",
       "             'suspicions': [0],\n",
       "             'obviously': [0],\n",
       "             'spends': [0],\n",
       "             'every': [0],\n",
       "             'nonworking': [0],\n",
       "             'hour': [0],\n",
       "             'thorough': [0],\n",
       "             'exploration': [0],\n",
       "             'things': [0, 3, 3],\n",
       "             'culinary': [0],\n",
       "             'desperation': [0],\n",
       "             'reach': [0],\n",
       "             'phone': [0],\n",
       "             'fable': [1],\n",
       "             'ant': [1, 1, 1],\n",
       "             'cricket': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "             'upon': [1, 2, 2, 3, 3],\n",
       "             'one': [1, 1, 1, 1, 2, 2, 3, 3, 3],\n",
       "             'hot': [1, 3],\n",
       "             'summer': [1, 1, 1, 1, 1],\n",
       "             'sang': [1, 1],\n",
       "             'cheerfully': [1],\n",
       "             'branch': [1],\n",
       "             'tree': [1, 1, 3, 3, 3, 3, 3],\n",
       "             'long': [1],\n",
       "             'line': [1],\n",
       "             'ants': [1, 1, 1, 1],\n",
       "             'struggled': [1],\n",
       "             'damely': [1],\n",
       "             'weight': [1],\n",
       "             'load': [1],\n",
       "             'grains': [1],\n",
       "             'song': [1, 1, 1],\n",
       "             'next': [1],\n",
       "             'spoke': [1],\n",
       "             'working': [1],\n",
       "             'hard': [1, 1],\n",
       "             'come': [1, 1, 1, 2, 2, 2],\n",
       "             'shade': [1],\n",
       "             'away': [1, 1, 3],\n",
       "             'sun': [1, 1],\n",
       "             'sing': [1, 1],\n",
       "             'tireless': [1],\n",
       "             'went': [1, 2],\n",
       "             'work': [1, 1, 3],\n",
       "             'ca': [1],\n",
       "             'nt': [1, 3, 3],\n",
       "             'said': [1, 1, 2, 2, 2, 2, 2, 3, 3, 3],\n",
       "             'must': [1, 2],\n",
       "             'store': [1],\n",
       "             'food': [1, 1, 2, 2],\n",
       "             'winter': [1, 1, 1, 1, 1],\n",
       "             'weather': [1],\n",
       "             'cold': [1, 1, 1, 2],\n",
       "             'ground': [1, 1, 2],\n",
       "             'white': [1, 1],\n",
       "             'snow': [1, 1, 1],\n",
       "             'nothing': [1, 1, 2],\n",
       "             'eat': [1, 1, 2],\n",
       "             'survive': [1],\n",
       "             'pantry': [1, 1],\n",
       "             'full': [1],\n",
       "             'plenty': [1],\n",
       "             'replied': [1, 2, 2, 3],\n",
       "             'lots': [1],\n",
       "             'anione': [1],\n",
       "             'heat': [1],\n",
       "             'laboured': [1],\n",
       "             'days': [1, 2],\n",
       "             'turned': [1, 1],\n",
       "             'months': [1],\n",
       "             'autumn': [1],\n",
       "             'came': [1, 3, 3],\n",
       "             'leaves': [1, 1],\n",
       "             'began': [1, 3, 3],\n",
       "             'fall': [1],\n",
       "             'left': [1, 1],\n",
       "             'bare': [1],\n",
       "             'grass': [1],\n",
       "             'turning': [1],\n",
       "             'thun': [1],\n",
       "             'woke': [1],\n",
       "             'shivering': [1],\n",
       "             'early': [1],\n",
       "             'frost': [1],\n",
       "             'tinged': [1],\n",
       "             'fields': [1],\n",
       "             'green': [1],\n",
       "             'brown': [1],\n",
       "             'wandered': [1],\n",
       "             'feeding': [1],\n",
       "             'dry': [1],\n",
       "             'stalks': [1],\n",
       "             'frozen': [1],\n",
       "             'fell': [1],\n",
       "             'find': [1],\n",
       "             'trembling': [1],\n",
       "             'famished': [1],\n",
       "             'thought': [1, 3, 3],\n",
       "             'sadly': [1, 3],\n",
       "             'warmth': [1],\n",
       "             'songs': [1],\n",
       "             'evening': [1, 3, 3],\n",
       "             'saw': [1],\n",
       "             'speck': [1],\n",
       "             'light': [1],\n",
       "             'distance': [1],\n",
       "             'trampling': [1],\n",
       "             'thick': [1, 3],\n",
       "             'made': [1],\n",
       "             'way': [1, 2, 3],\n",
       "             'towards': [1],\n",
       "             'open': [1, 1, 3],\n",
       "             'door': [1, 1],\n",
       "             'please': [1],\n",
       "             'starving': [1],\n",
       "             'give': [1, 3],\n",
       "             'leant': [1],\n",
       "             'window': [1],\n",
       "             'hungry': [1],\n",
       "             'roof': [1],\n",
       "             'ah': [1],\n",
       "             'yes': [1, 3, 3],\n",
       "             'remember': [1],\n",
       "             'getting': [1],\n",
       "             'ready': [1],\n",
       "             'singing': [1, 1, 3],\n",
       "             'filling': [1],\n",
       "             'whole': [1],\n",
       "             'earth': [1],\n",
       "             'sky': [1],\n",
       "             'eh': [1],\n",
       "             'well': [1, 3],\n",
       "             'dancing': [1],\n",
       "             'seven': [2, 2, 2, 2, 2, 2],\n",
       "             'old': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "             'samurai': [2, 2, 2, 2, 2, 2],\n",
       "             'far': [2, 2],\n",
       "             'japan': [2],\n",
       "             'band': [2],\n",
       "             'fierce': [2],\n",
       "             'robbers': [2, 2, 2, 2, 2, 2],\n",
       "             'hiding': [2],\n",
       "             'place': [2],\n",
       "             'top': [2, 3],\n",
       "             'mountain': [2, 2, 2, 2],\n",
       "             'almost': [2],\n",
       "             'always': [2, 2, 3],\n",
       "             'covered': [2],\n",
       "             'grey': [2],\n",
       "             'clouds': [2],\n",
       "             'windswept': [2],\n",
       "             'battered': [2],\n",
       "             'storms': [2],\n",
       "             'lived': [2, 3],\n",
       "             'large': [2],\n",
       "             'cave': [2, 2],\n",
       "             'piled': [2],\n",
       "             'spoils': [2],\n",
       "             'attacked': [2],\n",
       "             'village': [2],\n",
       "             'murdered': [2],\n",
       "             'poor': [2, 3, 3],\n",
       "             'folk': [2],\n",
       "             'chanced': [2],\n",
       "             'stole': [2],\n",
       "             'whatever': [2],\n",
       "             'lay': [2, 2],\n",
       "             'hands': [2],\n",
       "             'burned': [2],\n",
       "             'wherever': [2],\n",
       "             'passed': [2],\n",
       "             'smoking': [2],\n",
       "             'ruins': [2],\n",
       "             'weeping': [2],\n",
       "             'men': [2, 2, 2, 2],\n",
       "             'women': [2],\n",
       "             'misery': [2],\n",
       "             'mournlng': [2],\n",
       "             'desolation': [2],\n",
       "             'emperor': [2, 2, 2, 2],\n",
       "             'worried': [2],\n",
       "             'sent': [2, 2],\n",
       "             'soldiers': [2],\n",
       "             'attack': [2],\n",
       "             'managed': [2],\n",
       "             'drive': [2],\n",
       "             'remaining': [2],\n",
       "             'raiko': [2, 2, 2, 2, 2, 2],\n",
       "             'served': [2, 2],\n",
       "             'many': [2],\n",
       "             'years': [2],\n",
       "             'bidding': [2],\n",
       "             'go': [2, 2, 2, 3],\n",
       "             'army': [2, 2],\n",
       "             'wipe': [2],\n",
       "             'bloodthirsty': [2],\n",
       "             'bandits': [2, 2, 2],\n",
       "             'sighed': [2, 3],\n",
       "             'majesty': [2],\n",
       "             'young': [2],\n",
       "             'alone': [2],\n",
       "             'today': [2],\n",
       "             'command': [2],\n",
       "             'submit': [2],\n",
       "             'force': [2],\n",
       "             'marauding': [2],\n",
       "             'six': [2],\n",
       "             'like': [2, 2, 3],\n",
       "             'help': [2],\n",
       "             'faith': [2],\n",
       "             'later': [2, 2, 3],\n",
       "             'set': [2, 3],\n",
       "             'journey': [2],\n",
       "             'horses': [2],\n",
       "             'swords': [2],\n",
       "             'shields': [2],\n",
       "             'armour': [2],\n",
       "             'longer': [2],\n",
       "             'worn': [2],\n",
       "             'anyway': [2],\n",
       "             'dressed': [2],\n",
       "             'humble': [2],\n",
       "             'pilgrims': [2, 2],\n",
       "             'summit': [2],\n",
       "             'watched': [2],\n",
       "             'leader': [2, 2],\n",
       "             'cares': [2],\n",
       "             'beggars': [2],\n",
       "             'let': [2, 2],\n",
       "             'climb': [2],\n",
       "             'reached': [2],\n",
       "             'humbly': [2],\n",
       "             'outside': [2, 3],\n",
       "             'wind': [2, 2],\n",
       "             'blowing': [2],\n",
       "             'see': [2, 3],\n",
       "             'trouble': [2],\n",
       "             'gang': [2],\n",
       "             'scornfully': [2],\n",
       "             'stay': [2],\n",
       "             'corner': [2, 2],\n",
       "             'huddled': [2],\n",
       "             'ate': [2],\n",
       "             'meal': [2, 3],\n",
       "             'stolen': [2],\n",
       "             'villages': [2],\n",
       "             'nearby': [2],\n",
       "             'threw': [2],\n",
       "             'scraps': [2],\n",
       "             'leftovers': [2],\n",
       "             'saying': [2, 2, 3],\n",
       "             'much': [2],\n",
       "             'good': [2],\n",
       "             'hours': [2],\n",
       "             'rose': [2],\n",
       "             'feet': [2],\n",
       "             'dropped': [2, 3],\n",
       "             'hospitality': [2],\n",
       "             'would': [2, 3, 3, 3, 3],\n",
       "             'offer': [2],\n",
       "             'liqueur': [2],\n",
       "             'sake': [2, 2],\n",
       "             'rice': [2],\n",
       "             'wine': [2, 3, 3],\n",
       "             'drink': [2],\n",
       "             'health': [2],\n",
       "             'needed': [2],\n",
       "             'second': [2],\n",
       "             'telling': [2],\n",
       "             'blink': [2, 2],\n",
       "             'eye': [2, 2],\n",
       "             'emptied': [2],\n",
       "             'goatskin': [2],\n",
       "             'bottle': [2],\n",
       "             'raika': [2],\n",
       "             'held': [2],\n",
       "             'dead': [2],\n",
       "             'contained': [2],\n",
       "             'potent': [2],\n",
       "             'poison': [2],\n",
       "             'wield': [2],\n",
       "             'sword': [2],\n",
       "             'three': [3, 3, 3, 3],\n",
       "             'wishes': [3, 3, 3, 3, 3],\n",
       "             'woodcutter': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "             'happily': [3],\n",
       "             'wife': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "             'pretty': [3],\n",
       "             'little': [3],\n",
       "             'log': [3],\n",
       "             'cabin': [3],\n",
       "             'middle': [3],\n",
       "             'forest': [3, 3],\n",
       "             'home': [3, 3],\n",
       "             'plate': [3],\n",
       "             'steaming': [3],\n",
       "             'soup': [3],\n",
       "             'waiting': [3],\n",
       "             'day': [3],\n",
       "             'strange': [3, 3],\n",
       "             'surprise': [3],\n",
       "             'big': [3, 3],\n",
       "             'fir': [3],\n",
       "             'holes': [3],\n",
       "             'trunk': [3],\n",
       "             'looked': [3],\n",
       "             'somehow': [3],\n",
       "             'different': [3],\n",
       "             'trees': [3, 3],\n",
       "             'chop': [3, 3],\n",
       "             'alarmed': [3],\n",
       "             'face': [3],\n",
       "             'elf': [3, 3, 3, 3, 3, 3, 3],\n",
       "             'popped': [3],\n",
       "             'banging': [3],\n",
       "             'asked': [3],\n",
       "             'thinking': [3, 3, 3, 3],\n",
       "             'cutting': [3],\n",
       "             'live': [3],\n",
       "             'axe': [3],\n",
       "             'astonlshment': [3],\n",
       "             'stammered': [3],\n",
       "             'pick': [3],\n",
       "             'lucky': [3],\n",
       "             'found': [3],\n",
       "             'homeless': [3],\n",
       "             'taken': [3],\n",
       "             'aback': [3],\n",
       "             'words': [3],\n",
       "             'qulckly': [3],\n",
       "             'recovered': [3],\n",
       "             'tiny': [3],\n",
       "             'hefty': [3],\n",
       "             'chap': [3],\n",
       "             'boldly': [3],\n",
       "             'cut': [3, 3],\n",
       "             'right': [3, 3],\n",
       "             'broke': [3],\n",
       "             'shall': [3, 3],\n",
       "             'put': [3, 3],\n",
       "             'grant': [3],\n",
       "             'agreed': [3],\n",
       "             'scratched': [3],\n",
       "             'say': [3],\n",
       "             'agree': [3],\n",
       "             'hack': [3],\n",
       "             'worked': [3],\n",
       "             'sweated': [3],\n",
       "             'task': [3],\n",
       "             'kept': [3],\n",
       "             'magic': [3],\n",
       "             'thinks': [3],\n",
       "             'busily': [3],\n",
       "             'cleaning': [3],\n",
       "             'pot': [3],\n",
       "             'house': [3],\n",
       "             'husband': [3, 3, 3, 3, 3],\n",
       "             'arrived': [3],\n",
       "             'grabbing': [3],\n",
       "             'round': [3],\n",
       "             'waist': [3],\n",
       "             'twirled': [3],\n",
       "             'delight': [3],\n",
       "             'hooray': [3, 3],\n",
       "             'luck': [3],\n",
       "             'woman': [3, 3, 3, 3],\n",
       "             'understand': [3],\n",
       "             'pleased': [3],\n",
       "             'shrugged': [3],\n",
       "             'free': [3],\n",
       "             'glass': [3, 3],\n",
       "             'fine': [3],\n",
       "             'table': [3],\n",
       "             'meeting': [3, 3],\n",
       "             'picture': [3],\n",
       "             'wonderful': [3],\n",
       "             'might': [3, 3],\n",
       "             'took': [3],\n",
       "             'first': [3],\n",
       "             'sip': [3],\n",
       "             'nice': [3],\n",
       "             'smacking': [3],\n",
       "             'lips': [3],\n",
       "             'wish': [3, 3, 3, 3, 3],\n",
       "             'string': [3, 3, 3],\n",
       "             'sausages': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "             'though': [3],\n",
       "             'instantly': [3, 3],\n",
       "             'tongue': [3, 3],\n",
       "             'stuttered': [3],\n",
       "             'rage': [3, 3],\n",
       "             'done': [3, 3, 3],\n",
       "             'stupid': [3],\n",
       "             'waste': [3],\n",
       "             'foollsh': [3],\n",
       "             'stick': [3],\n",
       "             'nose': [3, 3, 3, 3, 3, 3],\n",
       "             'sooner': [3],\n",
       "             'sald': [3],\n",
       "             'leapt': [3],\n",
       "             'stuck': [3],\n",
       "             'fast': [3],\n",
       "             'end': [3, 3],\n",
       "             'flew': [3],\n",
       "             'idiot': [3],\n",
       "             'wished': [3, 3],\n",
       "             'mortified': [3],\n",
       "             'repeated': [3],\n",
       "             'mistake': [3],\n",
       "             'exclaimed': [3, 3],\n",
       "             'luckily': [3],\n",
       "             'stopped': [3],\n",
       "             'realizing': [3],\n",
       "             'horror': [3],\n",
       "             'point': [3],\n",
       "             'chopped': [3],\n",
       "             'complained': [3],\n",
       "             'blamed': [3],\n",
       "             'burst': [3],\n",
       "             'laughing': [3],\n",
       "             'knew': [3],\n",
       "             'look': [3],\n",
       "             'really': [3],\n",
       "             'looks': [3],\n",
       "             'tried': [3],\n",
       "             'tug': [3],\n",
       "             'budge': [3],\n",
       "             'pulled': [3, 3],\n",
       "             'vain': [3],\n",
       "             'firmly': [3],\n",
       "             'attached': [3],\n",
       "             'terrified': [3],\n",
       "             'rest': [3],\n",
       "             'feeling': [3],\n",
       "             'sorry': [3],\n",
       "             'wondering': [3],\n",
       "             'awkward': [3],\n",
       "             'grasping': [3],\n",
       "             'tugged': [3],\n",
       "             'simply': [3],\n",
       "             'pair': [3],\n",
       "             'sat': [3],\n",
       "             'floor': [3],\n",
       "             'gazing': [3],\n",
       "             'thing': [3],\n",
       "             'ventured': [3],\n",
       "             'timidly': [3],\n",
       "             'afraid': [3],\n",
       "             'remembering': [3],\n",
       "             'dreams': [3],\n",
       "             'riches': [3],\n",
       "             'bravely': [3],\n",
       "             'third': [3],\n",
       "             'leave': [3],\n",
       "             'hugged': [3],\n",
       "             'tearfully': [3],\n",
       "             'maybe': [3],\n",
       "             'happy': [3],\n",
       "             'reminder': [3],\n",
       "             'couple': [3],\n",
       "             'fried': [3],\n",
       "             'gloomily': [3],\n",
       "             'cost': [3]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(list): \n",
    "      \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for items in my_list: \n",
    "        freq[items] = my_list.count(items) \n",
    "        \n",
    "        \n",
    "    for key, value in freq.items(): \n",
    "        print (\"% d : % d\"%(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: last\n",
      "The given term occurs in the document number:\n",
      "{0, 1, 2, 3}\n",
      " \n",
      "Occurence of term in document number and their frequency:\n",
      "{0, 1, 2, 3}\n",
      " 0 :  1\n",
      " 1 :  2\n",
      " 2 :  3\n",
      " 3 :  1\n",
      " \n",
      "\n",
      "Query: cricket\n",
      "The given term occurs in the document number:\n",
      "{1}\n",
      " \n",
      "Occurence of term in document number and their frequency:\n",
      "{1}\n",
      " 1 :  10\n"
     ]
    }
   ],
   "source": [
    "query1 = \"last\"\n",
    "print (\"Query: \"+ query1)\n",
    "my_list = inv_indx[query1]\n",
    "documentoccur = set(my_list)\n",
    "\n",
    "print (\"The given term occurs in the document number:\")\n",
    "print (documentoccur)\n",
    "print(\" \")\n",
    "print(\"Occurence of term in document number and their frequency:\") \n",
    "print(documentoccur)\n",
    "result = CountFrequency(my_list)\n",
    "\n",
    "print (\" \\n\")\n",
    "query2 = \"cricket\"\n",
    "print (\"Query: \"+ query2)\n",
    "my_list = inv_indx[query2]\n",
    "documentoccur = set(my_list)\n",
    "\n",
    "print (\"The given term occurs in the document number:\")\n",
    "print (documentoccur)\n",
    "print(\" \")\n",
    "print(\"Occurence of term in document number and their frequency:\") \n",
    "print(documentoccur)\n",
    "result = CountFrequency(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "files_with_index = {'1:\"aircon\"', '2:\"antcrick\"', '3:\"7oldsamr\"', '4:\"3wishes\"'}\n",
    "unique_words_all = set(data1).union(set(data2)).union(set(data3)).union(set(data4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_all_unique_words_and_freq(words):\n",
    "    words_unique = []\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word not in words_unique:\n",
    "            words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "        word_freq[word] = words.count(word)\n",
    "    return word_freq\n",
    "def finding_freq_of_word_in_doc(word,words):\n",
    "    freq = words.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self ,docId, freq = None):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.nextval = None\n",
    "    \n",
    "class SlinkedList:\n",
    "    def __init__(self ,head = None):\n",
    "        self.head = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_list_data = {}\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = SlinkedList()\n",
    "    linked_list_data[word].head = Node(1,Node)\n",
    "word_freq_in_doc = {}\n",
    "idx = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    words = item\n",
    "    words = [word for word in words if len(words)>1]\n",
    "    words = [word.lower() for word in words]\n",
    "    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "    for word in word_freq_in_doc.keys():\n",
    "        linked_list = linked_list_data[word].head\n",
    "        while linked_list.nextval is not None:\n",
    "            linked_list = linked_list.nextval\n",
    "        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query:local AND supply\n",
      "['and']\n",
      "local\n",
      "supply\n",
      "[[1, 0, 0, 0], [1, 0, 0, 0]]\n",
      "[[1, 0, 0, 0], [1, 0, 0, 0]]\n",
      "Present in Document No:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter your query:')\n",
    "query = word_tokenize(query)\n",
    "connecting_words = []\n",
    "cnt = 1\n",
    "different_words = []\n",
    "for word in query:\n",
    "    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "        different_words.append(word.lower())\n",
    "    else:\n",
    "        connecting_words.append(word.lower())\n",
    "print(connecting_words)\n",
    "total_files = len(files_with_index)\n",
    "zeroes_and_ones = []\n",
    "zeroes_and_ones_of_all_words = []\n",
    "for word in (different_words):\n",
    "    if word.lower() in unique_words_all:\n",
    "        zeroes_and_ones = [0] * total_files\n",
    "        linkedlist = linked_list_data[word].head\n",
    "        print(word)\n",
    "        while linkedlist.nextval is not None:\n",
    "            zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n",
    "            linkedlist = linkedlist.nextval\n",
    "        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n",
    "    else:\n",
    "        print(word,\" not found\")\n",
    "        sys.exit()\n",
    "print(zeroes_and_ones_of_all_words)\n",
    "for word in connecting_words:\n",
    "    word_list1 = zeroes_and_ones_of_all_words[0]\n",
    "    word_list2 = zeroes_and_ones_of_all_words[1]\n",
    "    if word == \"and\":\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "    elif word == \"or\":\n",
    "        bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "    elif word == \"not\":\n",
    "        bitwise_op = [not w1 for w1 in word_list2]\n",
    "        bitwise_op = [int(b == True) for b in bitwise_op]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n",
    "zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "        \n",
    "files = []    \n",
    "print(zeroes_and_ones_of_all_words)\n",
    "lis = zeroes_and_ones_of_all_words[0]\n",
    "cnt = 4\n",
    "for index in lis:\n",
    "    if index == 1:\n",
    "        docid = index + 1\n",
    "        print(\"Present in Document No:\")\n",
    "        print(docid)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "#https://medium.com/@adriensieg/text-similarities-da019229c894"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Coefficient Short Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who cares about seven beggars\".split()\n",
    "jaccard_similarity(query, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who cares about seven beggars\".split()\n",
    "jaccard_similarity(query, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017543859649122806"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who cares about seven beggars\".split()\n",
    "jaccard_similarity(query, data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who cares about seven beggars\".split()\n",
    "jaccard_similarity(query, data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Coefficient Long Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"An early frost tinged the fields with white and turned the last of the green leaves brown\"\n",
    "jaccard_similarity(query, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"An early frost tinged the fields with white and turned the last of the green leaves brown\"\n",
    "jaccard_similarity(query, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"An early frost tinged the fields with white and turned the last of the green leaves brown\"\n",
    "jaccard_similarity(query, data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"An early frost tinged the fields with white and turned the last of the green leaves brown\"\n",
    "jaccard_similarity(query, data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'corpora', 'interfaces', 'logger', 'logging', 'matutils', 'models', 'parsing', 'scripts', 'similarities', 'summarization', 'topic_coherence', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(dir(gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in dictionary: 606\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in dictionary:\",len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in data]\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=4, num_nnz=694)\n",
      "694\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 4 documents in 0 shards (stored under C:/Users/Inaam/Desktop/NLP/Similarity/sims)\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity('C:/Users/Inaam/Desktop/NLP/Similarity/sims',tf_idf[corpus], num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Short Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'cares', 'about', 'seven', 'beggars', '.']\n",
      "[(288, 1), (295, 1), (388, 1)]\n",
      "[(288, 0.5773502691896258), (295, 0.5773502691896258), (388, 0.5773502691896258)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"Who cares about seven beggars.\")]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.20997019, 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Query result:')\n",
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF predict correctly with score 0.209, the text is from story 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Long Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'early', 'frost', 'tinged', 'the', 'fields', 'with', 'white', 'and', 'turned', 'the', 'last', 'of', 'the', 'green', 'leaves', 'brown']\n",
      "[(68, 1), (164, 1), (177, 1), (187, 1), (191, 1), (198, 1), (206, 1), (252, 1), (258, 1), (268, 1)]\n",
      "[(164, 0.3333333333333333), (177, 0.3333333333333333), (187, 0.3333333333333333), (191, 0.3333333333333333), (198, 0.3333333333333333), (206, 0.3333333333333333), (252, 0.3333333333333333), (258, 0.3333333333333333), (268, 0.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"An early frost tinged the fields with white and turned the last of the green leaves brown\")]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.22125918, 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Query result:')\n",
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF predict correctly with score 0.221, the text is from story 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"An early frost tinged the fields with white and turned the last of the green leaves brown\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = set(query).union(set(data1)).union(set(data2)).union(set(data3)).union(set(data4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryDict = dict.fromkeys(wordSet, 0)\n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "wordDictC = dict.fromkeys(wordSet, 0)\n",
    "wordDictD = dict.fromkeys(wordSet, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in query:\n",
    "    queryDict[word]+=1\n",
    "    \n",
    "for word in data1:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in data2:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "for word in data3:\n",
    "    wordDictC[word]+=1\n",
    "    \n",
    "for word in data4:\n",
    "    wordDictD[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>An</th>\n",
       "      <th>aback</th>\n",
       "      <th>active</th>\n",
       "      <th>adjusts</th>\n",
       "      <th>afraid</th>\n",
       "      <th>agree</th>\n",
       "      <th>agreed</th>\n",
       "      <th>ah</th>\n",
       "      <th>air</th>\n",
       "      <th>alarmed</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>working</th>\n",
       "      <th>worn</th>\n",
       "      <th>worried</th>\n",
       "      <th>would</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   An  aback  active  adjusts  afraid  agree  agreed  ah  air  alarmed  ...  \\\n",
       "0   1      0       0        0       0      0       0   0    0        0  ...   \n",
       "1   0      0       1        1       0      0       0   0    3        0  ...   \n",
       "2   0      0       0        0       0      0       0   1    0        0  ...   \n",
       "3   0      0       0        0       0      0       0   0    0        0  ...   \n",
       "4   0      1       0        0       1      1       1   0    1        1  ...   \n",
       "\n",
       "   work  worked  working  worn  worried  would  years  yellow  yes  young  \n",
       "0     0       0        0     0        0      0      0       0    0      0  \n",
       "1     0       0        0     0        0      0      0       1    0      0  \n",
       "2     2       0        1     0        0      0      0       1    1      0  \n",
       "3     0       0        0     1        1      1      1       0    0      1  \n",
       "4     1       1        0     0        0      4      0       0    2      0  \n",
       "\n",
       "[5 rows x 611 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([queryDict, wordDictA, wordDictB, wordDictC, wordDictD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfquery = computeTF(queryDict, query)\n",
    "tfBowA = computeTF(wordDictA, data1)\n",
    "tfBowB = computeTF(wordDictB, data2)\n",
    "tfBowC = computeTF(wordDictC, data3)\n",
    "tfBowD = computeTF(wordDictD, data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([queryDict, wordDictA, wordDictB, wordDictC, wordDictD])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfquery = computeTFIDF(tfquery, idfs)\n",
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "tfidfBowC = computeTFIDF(tfBowC, idfs)\n",
    "tfidfBowD = computeTFIDF(tfBowD, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tfidfscore = pd.DataFrame([tfidfquery, tfidfBowA, tfidfBowB, tfidfBowC, tfidfBowD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>An</th>\n",
       "      <th>aback</th>\n",
       "      <th>active</th>\n",
       "      <th>adjusts</th>\n",
       "      <th>afraid</th>\n",
       "      <th>agree</th>\n",
       "      <th>agreed</th>\n",
       "      <th>ah</th>\n",
       "      <th>air</th>\n",
       "      <th>alarmed</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>working</th>\n",
       "      <th>worn</th>\n",
       "      <th>worried</th>\n",
       "      <th>would</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.041116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00404</td>\n",
       "      <td>0.00404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  611 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         An     aback   active  adjusts    afraid     agree    agreed  \\\n",
       "0  0.041116  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.00404  0.00404  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.001952  0.00000  0.00000  0.001952  0.001952  0.001952   \n",
       "\n",
       "         ah       air   alarmed  ...      work    worked   working      worn  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.006901  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.003738  0.000000  0.000000  ...  0.004256  0.000000  0.003738  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.002853   \n",
       "4  0.000000  0.001112  0.001952  ...  0.001112  0.001952  0.000000  0.000000   \n",
       "\n",
       "    worried     would     years    yellow       yes     young  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.002300  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.002128  0.002128  0.000000  \n",
       "3  0.002853  0.001624  0.002853  0.000000  0.000000  0.002853  \n",
       "4  0.000000  0.004446  0.000000  0.000000  0.002223  0.000000  \n",
       "\n",
       "[5 rows x 611 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity for Short Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortquery = \"who cares about seven beggars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.06972064, 0.        ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidfscore[0:1], tfidfscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The given text is from the third story \"THE SEVEN OLD SAMURAI\" which is correctly predicted with a similarity score of 0.069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity for Long Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "longquery = \"An early frost tinged the fields with white and turned the last of the green leaves brown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.05434862, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidfscore[0:1], tfidfscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"The Fable of the Ant and the Cricket\" is the second story which is correctly predicted by Cosine Similarity with a similarity score = 0.0543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
